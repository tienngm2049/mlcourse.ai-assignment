{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/webt/ia/m9/zk/iam9zkyzqebnf_okxipihkgjwnw.jpeg\" />\n",
    "    \n",
    "**<center>[mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course** </center><br>\n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). [mlcourse.ai](https://mlcourse.ai) is powered by [OpenDataScience (ods.ai)](https://ods.ai/) © 2017—2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6. Task</center><a class=\"tocSkip\">\n",
    "### <center> Beating benchmarks in \"How good is your Medium article?\"</center><a class=\"tocSkip\">\n",
    "    \n",
    "[Competition](https://www.kaggle.com/c/how-good-is-your-medium-article). The task is to beat \"Assignment 6 baseline\" (~1.45 Public LB score). You can refer to [this simple Ridge baseline](https://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline?rvi=1).\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel __#mlcourse_ai_eng__, pinned thread __#a6_bonus__. If you are sure that something is not 100% correct, please leave your feedback there*\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost nltk dill optuna lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from html.parser import HTMLParser\n",
    "from pathlib import Path\n",
    "\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "import pandas as pd\n",
    "from nltk import TweetTokenizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "PATH_TO_DATA = \"data\"\n",
    "PATH_TO_SAVE_DIR = \"prepared_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from an article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return \"\".join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:\n",
    "        result = json.loads(line)\n",
    "    except Exception as e:\n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(\" \")[-1].replace(\")\", \"\"))\n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = \" \"\n",
    "        new_line = \"\".join(new_line)\n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features `content`, `published`, `title` and `author`, write them to separate files for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file(filename):\n",
    "    file = Path(filename)\n",
    "    return file.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_disk(features, path_to_save, prefix):\n",
    "    reload_files = False\n",
    "    d = defaultdict()\n",
    "    for feature in features:\n",
    "        filename = os.path.join(path_to_save, prefix + \"_\" + feature + \".txt\")\n",
    "        if check_file(filename):\n",
    "            print(f\"Reading from disk {filename}\")\n",
    "            with open(filename, \"rb\") as fp:\n",
    "                d[feature] = pickle.load(fp)\n",
    "        else:\n",
    "            reload_files = True\n",
    "\n",
    "    return reload_files, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_write(path_to_data, path_to_save, inp_filename, is_train=True):\n",
    "    titles = []\n",
    "    contents = []\n",
    "    dates = []\n",
    "\n",
    "    authors = []\n",
    "    features = [\"content\", \"published\", \"title\", \"author\"]\n",
    "    prefix = \"train\" if is_train else \"test\"\n",
    "\n",
    "    Path(path_to_save).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    reload_files, d = read_from_disk(features, path_to_save, prefix)\n",
    "\n",
    "    if reload_files:\n",
    "\n",
    "        feature_files = [\n",
    "            open(os.path.join(path_to_save, \"{}_{}.txt\".format(prefix, feat)), \"w\", encoding=\"utf-8\")\n",
    "            for feat in features\n",
    "        ]\n",
    "\n",
    "        with open(os.path.join(path_to_data, inp_filename), encoding=\"utf-8\") as inp_json_file:\n",
    "\n",
    "            for line in tqdm(inp_json_file, desc=f\"Reading {prefix} json files\"):\n",
    "                json_data = read_json_line(line)\n",
    "\n",
    "                title = json_data[\"title\"].replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \").replace(\"\\xa0\", \" \")\n",
    "                content = strip_tags(\n",
    "                    json_data[\"content\"].replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \").replace(\"\\xa0\", \" \")\n",
    "                )\n",
    "                published = json_data[\"published\"]\n",
    "                author = json_data[\"meta_tags\"][\"author\"]\n",
    "                authors_name = json_data[\"meta_tags\"][\"author\"]\n",
    "\n",
    "                titles.append(title)\n",
    "                contents.append(content)\n",
    "                dates.append(published)\n",
    "                authors.append(authors_name)\n",
    "\n",
    "        d = {\"content\": contents, \"published\": dates, \"title\": titles, \"author\": authors}\n",
    "        for feature in features:\n",
    "            filename = prefix + \"_\" + feature + \".txt\"\n",
    "            with open(os.path.join(path_to_save, filename), \"wb\") as fp:\n",
    "                pickle.dump(d[feature], fp)\n",
    "    else:\n",
    "        titles, contents, dates, authors = d[\"title\"], d[\"content\"], d[\"published\"], d[\"author\"]\n",
    "\n",
    "    return titles, contents, dates, authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [competition data](https://www.kaggle.com/c/how-good-is-your-medium-article/data) and place it where it's convenient for you. You can modify the path to data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from disk prepared_data\\train_content.txt\n",
      "Reading from disk prepared_data\\train_published.txt\n",
      "Reading from disk prepared_data\\train_title.txt\n",
      "Reading from disk prepared_data\\train_author.txt\n",
      "Reading from disk prepared_data\\test_content.txt\n",
      "Reading from disk prepared_data\\test_published.txt\n",
      "Reading from disk prepared_data\\test_title.txt\n",
      "Reading from disk prepared_data\\test_author.txt\n"
     ]
    }
   ],
   "source": [
    "train_titles, train_contents, train_dates, train_authors = extract_features_and_write(\n",
    "    PATH_TO_DATA, PATH_TO_SAVE_DIR, \"train.json\", is_train=True\n",
    ")\n",
    "test_titles, test_contents, test_dates, test_authors = extract_features_and_write(\n",
    "    PATH_TO_DATA, PATH_TO_SAVE_DIR, \"test.json\", is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the following groups of features:**\n",
    "* Tf-Idf with article content:\n",
    "  * ngram_range=(1, 2)\n",
    "  * max_features=100000\n",
    "* Tf-Idf with article titles:\n",
    "  * ngram_range=(1, 2)\n",
    "  * max_features=100000\n",
    "* Time features: \n",
    "  * publication hour, \n",
    "  * time of the day \n",
    "  * weekend or not\n",
    "* Bag of authors  \n",
    "i.e. One-Hot-Encoded author names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_NGRAMS = (1, 2)  # for tf-idf on titles\n",
    "CONTENT_NGRAMS = (1, 2)  # for tf-idf on contents\n",
    "MAX_FEATURES = 100_000  # for tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "assert tokenizer.tokenize(\"Now I'm a man\") == [\"Now\", \"I'm\", \"a\", \"man\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing TF-IDF vectorization for articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_vec_params = {\n",
    "    \"ngram_range\": CONTENT_NGRAMS,\n",
    "    \"max_features\": MAX_FEATURES,\n",
    "    \"tokenizer\": tokenize,\n",
    "    \"stop_words\": ENGLISH_STOP_WORDS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vectorizer_params_str = f\"{art_vec_params['ngram_range']}_{art_vec_params['max_features']}\"\n",
    "article_vec_name = f\"vectorizer_article_{article_vectorizer_params_str}.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12min 25s\n",
      "Wall time: 12min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "article_vec_is_saved = check_file(article_vec_name)\n",
    "if article_vec_is_saved:\n",
    "    vectorizer_article = pickle.load(open(article_vec_name, \"rb\"))\n",
    "else:\n",
    "    vectorizer_article = TfidfVectorizer(**article_vectorizer_params)\n",
    "    vectorizer_article.fit(train_contents)\n",
    "    pickle.dump(vectorizer_article, open(article_vec_name, \"wb\"))\n",
    "\n",
    "X_train_article = vectorizer_article.transform(train_contents)\n",
    "X_test_article = vectorizer_article.transform(test_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing TF-IDF vectorization for titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vec_params = {\n",
    "    \"ngram_range\": TITLE_NGRAMS,\n",
    "    \"max_features\": MAX_FEATURES,\n",
    "    \"tokenizer\": tokenize,\n",
    "    \"stop_words\": ENGLISH_STOP_WORDS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vectorizer_params_str = f\"{title_vec_params['ngram_range']}_{title_vec_params['max_features']}\"\n",
    "title_vec_name = f\"vectorizer_title_{title_vectorizer_params_str}.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.27 s\n",
      "Wall time: 6.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "title_vec_is_saved = check_file(title_vec_name)\n",
    "if title_vec_is_saved:\n",
    "    vectorizer_title = pickle.load(open(title_vec_name, \"rb\"))\n",
    "else:\n",
    "    vectorizer_title = TfidfVectorizer(**title_vec_params)\n",
    "    vectorizer_title.fit(train_titles)\n",
    "    pickle.dump(vectorizer_title, open(title_vec_name, \"wb\"))\n",
    "\n",
    "X_train_title = vectorizer_title.transform(train_titles)\n",
    "X_test_title = vectorizer_title.transform(test_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features\n",
    "def add_time_features(dates):\n",
    "    scaler = StandardScaler()\n",
    "    hour = scaler.fit_transform(np.array([date.hour for date in dates]).reshape(-1, 1))\n",
    "    weekday = scaler.fit_transform(np.array([date.weekday() for date in dates]).reshape(-1, 1))\n",
    "    morning = scaler.fit_transform(((hour >= 7) & (hour <= 11)).astype(\"int\").reshape(-1, 1))\n",
    "    day = scaler.fit_transform(((hour >= 12) & (hour <= 18)).astype(\"int\").reshape(-1, 1))\n",
    "    evening = scaler.fit_transform(((hour >= 19) & (hour <= 23)).astype(\"int\").reshape(-1, 1))\n",
    "    night = scaler.fit_transform(((hour >= 0) & (hour <= 6)).astype(\"int\").reshape(-1, 1))\n",
    "    weekend_temp = np.array([date.weekday() for date in dates]).reshape(-1, 1)\n",
    "    weekend = scaler.fit_transform(((weekend_temp >= 5) & (weekend_temp <= 6)).astype(\"int\").reshape(-1, 1))\n",
    "\n",
    "    feature_names = [\"morning\", \"day\", \"evening\", \"night\", \"weekday\"]\n",
    "    time_features = pd.DataFrame(\n",
    "        list(zip(morning.flatten(), day.flatten(), evening.flatten(), night.flatten(), weekend.flatten())),\n",
    "        columns=feature_names,\n",
    "    )\n",
    "    sparse_time_features = csr_matrix(time_features.values)\n",
    "    return sparse_time_features, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times = pd.to_datetime([date[\"$date\"] for date in train_dates])\n",
    "test_times = pd.to_datetime([date[\"$date\"] for date in test_dates])\n",
    "\n",
    "X_train_time_features_sparse, time_feature_names = add_time_features(train_times)\n",
    "X_test_time_features_sparse, _ = add_time_features(test_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing bag of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = np.unique(train_authors + test_authors)\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "enc.fit(authors.reshape(-1, 1))\n",
    "enc.categories_\n",
    "X_train_author_sparse = enc.transform(np.array(train_authors).reshape(-1, 1)).toarray()\n",
    "X_test_author_sparse = enc.transform(np.array(test_authors).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = [len(article) for article in train_contents]\n",
    "test_len = [len(article) for article in test_contents]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_len_sparse = scaler.fit_transform(np.array(train_len).reshape(-1, 1))\n",
    "X_test_len_sparse = scaler.fit_transform(np.array(test_len).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join all sparse matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = hstack(\n",
    "    [X_train_article, X_train_title, X_train_author_sparse, X_train_time_features_sparse, X_train_len_sparse]\n",
    ").tocsr()\n",
    "\n",
    "X_test_sparse = hstack(\n",
    "    [X_test_article, X_test_title, X_test_author_sparse, X_test_time_features_sparse, X_test_len_sparse]\n",
    ").tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Saving dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_trapickle, open(\"X_train_sparse.pickle\", \"wb\"))\n",
    "pickle.dump(X_test_sparse, open(\"X_test_sparse.pickle\", \"wb\"))\n",
    "pickle.dump(y_train, open(\"y_train.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = pickle.load(open(\"X_train_sparse.pickle\", \"rb\"))\n",
    "X_test_sparse = pickle.load(open(\"X_test_sparse.pickle\", \"rb\"))\n",
    "y_train = pickle.load(open(\"y_train.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read train target and split data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, \"train_log1p_recommends.csv\"), index_col=\"id\")\n",
    "y_train = train_target[\"log_recommends\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "\n",
    "X_valid_sparse = X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      "{'alpha': 1.1253355826007645}\n"
     ]
    }
   ],
   "source": [
    "alpha_values = np.logspace(-2, 2, 40)\n",
    "# alpha_values = np.logspace(0, 0.1, 40)\n",
    "\n",
    "logit_grid_searcher = GridSearchCV(\n",
    "    estimator=Ridge(random_state=SEED),\n",
    "    param_grid={\"alpha\": alpha_values},\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=6,\n",
    "    cv=3,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "logit_grid_searcher.fit(X_train_sparse, np.log1p(y_train))\n",
    "\n",
    "print(logit_grid_searcher.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.29016979593377706"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_grid_searcher.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'alpha': 1.1867492730446552}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0619034952860282"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(random_state=SEED, alpha=1.1253355826007645)\n",
    "ridge.fit(X_train_part_sparse, np.log1p(y_train_part))\n",
    "ridge_valid_pred = np.expm1(ridge.predict(X_valid_sparse))\n",
    "ridge_valid_mae = mean_absolute_error(y_valid, ridge_valid_pred)\n",
    "ridge_valid_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(random_state=SEED, alpha=1.1253355826007645)\n",
    "ridge.fit(X_train_sparse, np.log1p(y_train))\n",
    "ridge_test_pred = np.expm1(ridge.predict(X_test_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/hamzaghanmi/xgboost-catboost-using-optuna?scriptVersionId=94510532  \n",
    "https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f  \n",
    "https://www.youtube.com/watch?v=5nYqK-HaoKY  \n",
    "https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook#2.-XGBoost-hyperparameters-  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-15 17:31:26,335]\u001b[0m A new study created in memory with name: no-name-c3191736-7363-4622-890c-393278331c71\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                                                          | 0/7 [00:00<?, ?it/s]C:\\Users\\ryblo\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\lightgbm\\engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's l1: 0.406432 + 0.019252\n",
      "[1]\tcv_agg's l1: 0.406432 + 0.019252\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tcv_agg's l1: 0.399635 + 0.0186409\n",
      "[2]\tcv_agg's l1: 0.399635 + 0.0186409\n",
      "[3]\tcv_agg's l1: 0.393892 + 0.0178621\n",
      "[3]\tcv_agg's l1: 0.393892 + 0.0178621\n",
      "[4]\tcv_agg's l1: 0.38884 + 0.0171225\n",
      "[4]\tcv_agg's l1: 0.38884 + 0.0171225\n",
      "[5]\tcv_agg's l1: 0.384564 + 0.0165255\n",
      "[5]\tcv_agg's l1: 0.384564 + 0.0165255\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5]\tcv_agg's l1: 0.384564 + 0.0165255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.384564:  14%|######2                                     | 1/7 [03:52<23:14, 232.42s/it]\u001b[32m[I 2022-05-15 17:35:22,019]\u001b[0m Trial 0 finished with value: 0.3845640013306222 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.3845640013306222.\u001b[0m\n",
      "feature_fraction, val_score: 0.384564:  14%|######2                                     | 1/7 [03:55<23:14, 232.42s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression_l1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     12\u001b[0m tuner \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLightGBMTunerCV(\n\u001b[0;32m     13\u001b[0m     params,\n\u001b[0;32m     14\u001b[0m     data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping(\u001b[38;5;241m10\u001b[39m), log_evaluation(\u001b[38;5;241m1\u001b[39m)],\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tuner\u001b[38;5;241m.\u001b[39mbest_score)\n\u001b[0;32m     21\u001b[0m best_params \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\optuna\\integration\\_lightgbm_tuner\\optimize.py:546\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;66;03m# Sampling.\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_train_set()\n\u001b[1;32m--> 546\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_feature_fraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtune_num_leaves()\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtune_bagging()\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\optuna\\integration\\_lightgbm_tuner\\optimize.py:571\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.tune_feature_fraction\u001b[1;34m(self, n_trials)\u001b[0m\n\u001b[0;32m    568\u001b[0m param_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, n_trials)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    570\u001b[0m sampler \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mGridSampler({param_name: param_values})\n\u001b[1;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tune_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparam_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature_fraction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\optuna\\integration\\_lightgbm_tuner\\optimize.py:654\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner._tune_params\u001b[1;34m(self, target_param_names, n_trials, sampler, step_name)\u001b[0m\n\u001b[0;32m    652\u001b[0m     _timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _n_trials \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 654\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_n_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optuna_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[0;32m    663\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\optuna\\integration\\_lightgbm_tuner\\optimize.py:320\u001b[0m, in \u001b[0;36m_OptunaObjectiveCV.__call__\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    318\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    319\u001b[0m train_set \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set)\n\u001b[1;32m--> 320\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlgbm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlgbm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m val_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cv_scores(cv_results)\n\u001b[0;32m    323\u001b[0m val_score \u001b[38;5;241m=\u001b[39m val_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\lightgbm\\engine.py:605\u001b[0m, in \u001b[0;36mcv\u001b[1;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks, eval_train_metric, return_cvbooster)\u001b[0m\n\u001b[0;32m    599\u001b[0m train_set\u001b[38;5;241m.\u001b[39m_update_params(params) \\\n\u001b[0;32m    600\u001b[0m          \u001b[38;5;241m.\u001b[39m_set_predictor(predictor) \\\n\u001b[0;32m    601\u001b[0m          \u001b[38;5;241m.\u001b[39mset_feature_name(feature_name) \\\n\u001b[0;32m    602\u001b[0m          \u001b[38;5;241m.\u001b[39mset_categorical_feature(categorical_feature)\n\u001b[0;32m    604\u001b[0m results \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m--> 605\u001b[0m cvfolds \u001b[38;5;241m=\u001b[39m \u001b[43m_make_n_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpreproc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfpreproc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstratified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m                        \u001b[49m\u001b[43meval_train_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_train_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# setup callbacks\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\lightgbm\\engine.py:361\u001b[0m, in \u001b[0;36m_make_n_folds\u001b[1;34m(full_data, folds, nfold, params, seed, fpreproc, stratified, shuffle, eval_train_metric)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_n_folds\u001b[39m(full_data, folds, nfold, params, seed, fpreproc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stratified\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    359\u001b[0m                   shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, eval_train_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124;03m\"\"\"Make a n-fold list of Booster from random indices.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     full_data \u001b[38;5;241m=\u001b[39m \u001b[43mfull_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m     num_data \u001b[38;5;241m=\u001b[39m full_data\u001b[38;5;241m.\u001b[39mnum_data()\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m folds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\lightgbm\\basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 1815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\lightgbm\\basic.py:1534\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     _safe_call(_LIB\u001b[38;5;241m.\u001b[39mLGBM_DatasetCreateFromFile(\n\u001b[0;32m   1529\u001b[0m         c_str(\u001b[38;5;28mstr\u001b[39m(data)),\n\u001b[0;32m   1530\u001b[0m         c_str(params_str),\n\u001b[0;32m   1531\u001b[0m         ref_dataset,\n\u001b[0;32m   1532\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)))\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix):\n\u001b[1;32m-> 1534\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__init_from_csr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsc_matrix):\n\u001b[0;32m   1536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_from_csc(data, params_str, ref_dataset)\n",
      "File \u001b[1;32m~\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\lightgbm\\basic.py:1728\u001b[0m, in \u001b[0;36mDataset.__init_from_csr\u001b[1;34m(self, csr, params_str, ref_dataset)\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m csr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m MAX_INT32\n\u001b[0;32m   1726\u001b[0m csr_indices \u001b[38;5;241m=\u001b[39m csr\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1728\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_DatasetCreateFromCSR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr_indptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_indptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsr_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "data = lgb.Dataset(X_train_sparse.astype(np.float32), label=np.log1p(y_train))\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression_l1\",\n",
    "    \"metric\": \"l1\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"device_type\": \"gpu\",\n",
    "}\n",
    "tuner = lgb.LightGBMTunerCV(\n",
    "    params,\n",
    "    data,\n",
    "    num_boost_round=5,\n",
    "    folds=KFold(n_splits=3),\n",
    "    callbacks=[early_stopping(10), log_evaluation(1)],\n",
    ")\n",
    "tuner.run()\n",
    "print(\"Best score:\", tuner.best_score)\n",
    "best_params = tuner.best_params\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"  Params: \")\n",
    "for key, value in best_params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryblo\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ryblo\\Documents\\projects\\mlcourse_ai_bonus_assignments\\venv\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.993373\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.992853\n",
      "[LightGBM] [Debug] init for col-wise cost 12.247353 seconds, init for row-wise cost 13.589792 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 15.266464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8541057\n",
      "[LightGBM] [Info] Number of data points in the train set: 62313, number of used features: 103682\n",
      "[LightGBM] [Info] Start training from score 1.291725\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[5]\tvalid_0's l1: 0.371947\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[10]\tvalid_0's l1: 0.363394\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[15]\tvalid_0's l1: 0.358158\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[20]\tvalid_0's l1: 0.353951\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[25]\tvalid_0's l1: 0.350329\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[30]\tvalid_0's l1: 0.347571\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[35]\tvalid_0's l1: 0.345086\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[40]\tvalid_0's l1: 0.343064\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[45]\tvalid_0's l1: 0.341309\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[50]\tvalid_0's l1: 0.339561\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[55]\tvalid_0's l1: 0.338115\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 8 and depth = 3\n",
      "[60]\tvalid_0's l1: 0.336539\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[60]\tvalid_0's l1: 0.336539\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_x_train = lgb.Dataset(X_train_sparse.astype(np.float32), label=np.log1p(y_train))\n",
    "lgb_x_valid = lgb.Dataset(X_valid_sparse.astype(np.float32), label=np.log1p(y_valid))\n",
    "param = {\"objective\": \"mean_absolute_error\", \"metric\": \"mae\", \"verbose\": 5, \"max_depth\": 9}\n",
    "bst_lgb = lgb.train(param, lgb_x_train, 60, verbose_eval=5, early_stopping_rounds=20, valid_sets=[lgb_x_valid])\n",
    "lgb_test_pred = np.expm1(bst_lgb.predict(X_test_sparse.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.242082401571958"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_valid_pred = np.expm1(bst_lgb.predict(X_valid_sparse))\n",
    "lgb_valid_mae = mean_absolute_error(y_valid, lgb_valid_pred)\n",
    "lgb_valid_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_valid, bins=30, alpha=.5, color='red', label='true', range=(0,10));\n",
    "plt.hist(ridge_valid_pred, bins=30, alpha=.5, color='green', label='Ridge', range=(0,10));\n",
    "plt.hist(lgb_valid_pred, bins=30, alpha=.5, color='blue', label='LGB', range=(0,10));\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.1495727794551638\n",
      "0.1 1.1254280584668486\n",
      "0.2 1.1044880003415622\n",
      "0.30000000000000004 1.0866146523065774\n",
      "0.4 1.07196704702466\n",
      "0.5 1.0606179818201866\n",
      "0.6000000000000001 1.0528472219070115\n",
      "0.7000000000000001 1.048980524373251\n",
      "0.8 1.0490509047268561\n",
      "0.9 1.0535718513461647\n",
      "1.0 1.0619034952860282\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0, 1.1, 0.1):\n",
    "    print(i, mean_absolute_error(y_valid, (1 - i) * lgb_valid_pred + i * ridge_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_TEST_TARGET = 4.33328  # what we got by submitting all zeros\n",
    "\n",
    "RIDGE_WEIGHT = 0.6 # weight of Ridge predictions in a blend with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_pred = (1 - RIDGE_WEIGHT) * lgb_test_pred + RIDGE_WEIGHT * ridge_test_pred\n",
    "mix_test_pred_modif = mix_pred + MEAN_TEST_TARGET - y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(\n",
    "    prediction,\n",
    "    filename,\n",
    "    path_to_sample=os.path.join(PATH_TO_DATA, \"sample_submission.csv\"),\n",
    "):\n",
    "    submission = pd.read_csv(path_to_sample, index_col=\"id\")\n",
    "\n",
    "    submission[\"log_recommends\"] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(mix_test_pred_modif, os.path.join(PATH_TO_DATA, \"assignment6_medium_submission.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now's the time for dirty Kaggle hacks. Form a submission file with all zeros. Make a submission. What do you get if you think about it? How is it going to help you with modifying your predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(\n",
    "    np.zeros_like(ridge_test_pred),\n",
    "    os.path.join(PATH_TO_DATA, \"medium_all_zeros_submission.csv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify predictions in an appropriate way (based on your all-zero submission) and make a new submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_test_pred_modif = ridge_test_pred\n",
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(\n",
    "    ridge_test_pred_modif,\n",
    "    os.path.join(PATH_TO_DATA, \"assignment6_medium_submission_with_hack.csv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the assignment. In case you'd like to try some more ideas for improvement:\n",
    "\n",
    "- Engineer good features, this is the key to success. Some simple features will be based on publication time, authors, content length and so on\n",
    "- You may not ignore HTML and extract some features from there\n",
    "- You'd better experiment with your validation scheme. You should see a correlation between your local improvements and LB score\n",
    "- Try TF-IDF, ngrams, Word2Vec and GloVe embeddings\n",
    "- Try various NLP techniques like stemming and lemmatization\n",
    "- Tune hyperparameters. In our example, we've left only 50k features and used C=1 as a regularization parameter, this can be changed\n",
    "- SGD and Vowpal Wabbit will train much faster\n",
    "- Play around with blending and/or stacking. An intro is given in [this Kernel](https://www.kaggle.com/kashnitsky/ridge-and-lightgbm-simple-blending) by @yorko \n",
    "- And neural nets of course. We don't cover them in this course byt still transformer-based architectures will likely perform well in such types of tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse_ai_bonus",
   "language": "python",
   "name": "mlcourse_ai_bonus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
