{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Feature Encoding:\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "In most data science problems, our datasets will contain categorical features. Categorical features contain a finite number of discrete values. How we represent these features will have an impact on the performance of our model. Like in other aspects of machine learning, there are no silver bullets. Determining the correct approach, specific to our model and data is part of the challenge.\n",
    "\n",
    "This tutorial aims to cover a few of these methods. We begin by covering a straight-forward technique before tackling more complicated lesser-known approaches.\n",
    "\n",
    "**List of methods covered**:\n",
    "1. One-Hot Encoding\n",
    "2. Feature Hashing\n",
    "3. Binary Encoding\n",
    "4. Target Encoding\n",
    "5. Weight of Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'sampleSubmission.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Set our random seed:\n",
    "SEED = 17\n",
    "PATH_TO_DIR = '../input/amazoncom-employee-access-challenge/'\n",
    "\n",
    "print(os.listdir(PATH_TO_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will be using the '[Amazon.com Employee Access Challenge](https://www.kaggle.com/c/amazon-employee-access-challenge)' dataset. This binary classification dataset is made up of strictly categorical features, which are already converted into numerals, making it a particularly suitable choice to explore various encoding techniques. To simplify things we will only be using a subset of the features for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Import data:\n",
    "train = pd.read_csv(PATH_TO_DIR + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['ACTION']\n",
    "train = train[['RESOURCE', 'MGR_ID', 'ROLE_FAMILY_DESC', 'ROLE_FAMILY', 'ROLE_CODE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare differences of these encoding methods on both a linear model and tree-based model. These represent two families of models which have contrasting behaviours when it comes to different feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state=SEED)\n",
    "rf = RandomForestClassifier(random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation subsets:\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a helper function to get the scores for each encoding method:\n",
    "def get_score(model, X, y, X_val, y_val):\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict_proba(X_val)[:,1]\n",
    "    score = roc_auc_score(y_val, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RESOURCE</th>\n",
       "      <th>MGR_ID</th>\n",
       "      <th>ROLE_FAMILY_DESC</th>\n",
       "      <th>ROLE_FAMILY</th>\n",
       "      <th>ROLE_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22434</th>\n",
       "      <td>29071</td>\n",
       "      <td>5178</td>\n",
       "      <td>158101</td>\n",
       "      <td>118424</td>\n",
       "      <td>118828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24163</th>\n",
       "      <td>20222</td>\n",
       "      <td>5550</td>\n",
       "      <td>119235</td>\n",
       "      <td>292795</td>\n",
       "      <td>118997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10066</th>\n",
       "      <td>79092</td>\n",
       "      <td>6047</td>\n",
       "      <td>279443</td>\n",
       "      <td>308574</td>\n",
       "      <td>118779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19869</th>\n",
       "      <td>14570</td>\n",
       "      <td>51104</td>\n",
       "      <td>189996</td>\n",
       "      <td>19721</td>\n",
       "      <td>118570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>32642</td>\n",
       "      <td>18097</td>\n",
       "      <td>130662</td>\n",
       "      <td>292795</td>\n",
       "      <td>117948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       RESOURCE  MGR_ID  ROLE_FAMILY_DESC  ROLE_FAMILY  ROLE_CODE\n",
       "22434     29071    5178            158101       118424     118828\n",
       "24163     20222    5550            119235       292795     118997\n",
       "10066     79092    6047            279443       308574     118779\n",
       "19869     14570   51104            189996        19721     118570\n",
       "1855      32642   18097            130662       292795     117948"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets have a quick look at our data:\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26215 entries, 22434 to 10863\n",
      "Data columns (total 5 columns):\n",
      "RESOURCE            26215 non-null int64\n",
      "MGR_ID              26215 non-null int64\n",
      "ROLE_FAMILY_DESC    26215 non-null int64\n",
      "ROLE_FAMILY         26215 non-null int64\n",
      "ROLE_CODE           26215 non-null int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6711, 4062, 2201, 67, 337)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discover the number of categories within each categorical feature:\n",
    "len(X_train.RESOURCE.unique()), len(X_train.MGR_ID.unique()), len(X_train.ROLE_FAMILY_DESC.unique()), len(X_train.ROLE_FAMILY.unique()),len(X_train.ROLE_CODE.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of each categorical column name:\n",
    "columns = [i for i in X_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, lets have a look at the speed and performance of training these models without any feature encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score without feature engineering: 0.5350998573215817\n",
      "CPU times: user 72 ms, sys: 36 ms, total: 108 ms\n",
      "Wall time: 86.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline_logit_score = get_score(logit, X_train, y_train, X_val, y_val)\n",
    "print('Logistic Regression score without feature engineering:', baseline_logit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score without feature engineering: 0.7785989401031782\n",
      "CPU times: user 512 ms, sys: 172 ms, total: 684 ms\n",
      "Wall time: 489 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline_rf_score = get_score(rf, X_train, y_train, X_val, y_val)\n",
    "print('Random Forest score without feature engineering:', baseline_rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method we will be covering is one that no doubt will be familiar to you. One-hot encoding expands a categorical feature made up of m categories into m* distinct features with values of either 0 or 1.\n",
    "\n",
    "There are two ways of implementing one-hot encoding, either with pandas or scikit-learn. In this tutorial we have chosen to use the latter.\n",
    "\n",
    "*Actually, it is seen as more correct to expand m categories into (m - 1) distinct features. The reason for this is twofold. Firstly, if the values of (m - 1) features are known, the m-th feature can be inferred and secondly because including the m-th feature can cause certain linear models to become unstable. More on that can be found [here](https://www.algosome.com/articles/dummy-variable-trap-regression.html). In practice I think this depends on your model. Some non-linear models actually do better with m features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_enc = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: \n",
      " 5 \n",
      "\n",
      "Features after OHE: \n",
      " 13378\n"
     ]
    }
   ],
   "source": [
    "print('Original number of features: \\n', X_train.shape[1], \"\\n\")\n",
    "data_ohe_train = (one_hot_enc.fit_transform(X_train))\n",
    "data_ohe_val = (one_hot_enc.transform(X_val))\n",
    "print('Features after OHE: \\n', data_ohe_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score with one-hot encoding: 0.8658376997849282\n",
      "CPU times: user 1.12 s, sys: 404 ms, total: 1.52 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ohe_logit_score = get_score(logit, data_ohe_train, y_train, data_ohe_val, y_val)\n",
    "print('Logistic Regression score with one-hot encoding:', ohe_logit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score with one-hot encoding: 0.8151528170201999\n",
      "CPU times: user 3min 58s, sys: 4.14 s, total: 4min 2s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ohe_rf_score = get_score(rf, data_ohe_train, y_train, data_ohe_val, y_val)\n",
    "print('Random Forest score with one-hot encoding:', ohe_rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, while the performance of the model has improved, training took longer as well. This is due to the increase in the number of features. Computational costs are not the only problem associated with the increase in dimensions. A dataset with more features will require a model with more parameters which in turn will require more data to train these parameters. In many cases, such as kaggle competitions, the size of our data is fixed and as a result the dimensionality of our data should always be a concern.\n",
    "\n",
    "One way of dealing with high dimensionality is by compressing the features. Feature hashing, which we will be covering next, is an example of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Hashing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature hashing maps each category in a categorical feature to an integer within a pre-determined range. This output range is smaller than the input range so multiple categories may be mapped to the same integer. Feature hashing is very similar to one-hot encoding but with a control over the output dimensions.\n",
    "\n",
    "To implement feature hashing in python we can use category_encoder, a library containing sklearn compabitable category encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install category_encoders:\n",
    "# pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import HashingEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the output dimensions is controlled by the variable n_components. This can be treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_list = [100, 500, 1000, 5000, 10000]\n",
    "n_components_list_str = [str(i) for i in n_components_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_logit_scores = []\n",
    "\n",
    "# Iterate over different n_components:\n",
    "for n_components in n_components_list:\n",
    "    \n",
    "    hashing_enc = HashingEncoder(cols=columns, n_components=n_components).fit(X_train, y_train)\n",
    "    \n",
    "    X_train_hashing = hashing_enc.transform(X_train.reset_index(drop=True))\n",
    "    X_val_hashing = hashing_enc.transform(X_val.reset_index(drop=True))\n",
    "    \n",
    "    fe_logit_score = get_score(logit, X_train_hashing, y_train, X_val_hashing, y_val)\n",
    "    fh_logit_scores.append(fe_logit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_components_list_str, fh_logit_scores, linewidth=3)\n",
    "plt.title('n_compontents vs roc_auc for feature hashing with logistic regression')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('score')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, performance on the Logistic Regression model improves as the number of components increase. But let us have a look at the effect of reducing the dimensions has on a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_enc = HashingEncoder(cols=columns, n_components=10000).fit(X_train, y_train)\n",
    "\n",
    "X_train_hashing = hashing_enc.transform(X_train.reset_index(drop=True))\n",
    "X_val_hashing = hashing_enc.transform(X_val.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "      <th>col_11</th>\n",
       "      <th>col_12</th>\n",
       "      <th>col_13</th>\n",
       "      <th>col_14</th>\n",
       "      <th>col_15</th>\n",
       "      <th>col_16</th>\n",
       "      <th>col_17</th>\n",
       "      <th>col_18</th>\n",
       "      <th>col_19</th>\n",
       "      <th>col_20</th>\n",
       "      <th>col_21</th>\n",
       "      <th>col_22</th>\n",
       "      <th>col_23</th>\n",
       "      <th>col_24</th>\n",
       "      <th>col_25</th>\n",
       "      <th>col_26</th>\n",
       "      <th>col_27</th>\n",
       "      <th>col_28</th>\n",
       "      <th>col_29</th>\n",
       "      <th>col_30</th>\n",
       "      <th>col_31</th>\n",
       "      <th>col_32</th>\n",
       "      <th>col_33</th>\n",
       "      <th>col_34</th>\n",
       "      <th>col_35</th>\n",
       "      <th>col_36</th>\n",
       "      <th>col_37</th>\n",
       "      <th>col_38</th>\n",
       "      <th>col_39</th>\n",
       "      <th>...</th>\n",
       "      <th>col_9960</th>\n",
       "      <th>col_9961</th>\n",
       "      <th>col_9962</th>\n",
       "      <th>col_9963</th>\n",
       "      <th>col_9964</th>\n",
       "      <th>col_9965</th>\n",
       "      <th>col_9966</th>\n",
       "      <th>col_9967</th>\n",
       "      <th>col_9968</th>\n",
       "      <th>col_9969</th>\n",
       "      <th>col_9970</th>\n",
       "      <th>col_9971</th>\n",
       "      <th>col_9972</th>\n",
       "      <th>col_9973</th>\n",
       "      <th>col_9974</th>\n",
       "      <th>col_9975</th>\n",
       "      <th>col_9976</th>\n",
       "      <th>col_9977</th>\n",
       "      <th>col_9978</th>\n",
       "      <th>col_9979</th>\n",
       "      <th>col_9980</th>\n",
       "      <th>col_9981</th>\n",
       "      <th>col_9982</th>\n",
       "      <th>col_9983</th>\n",
       "      <th>col_9984</th>\n",
       "      <th>col_9985</th>\n",
       "      <th>col_9986</th>\n",
       "      <th>col_9987</th>\n",
       "      <th>col_9988</th>\n",
       "      <th>col_9989</th>\n",
       "      <th>col_9990</th>\n",
       "      <th>col_9991</th>\n",
       "      <th>col_9992</th>\n",
       "      <th>col_9993</th>\n",
       "      <th>col_9994</th>\n",
       "      <th>col_9995</th>\n",
       "      <th>col_9996</th>\n",
       "      <th>col_9997</th>\n",
       "      <th>col_9998</th>\n",
       "      <th>col_9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_0  col_1  col_2  col_3    ...     col_9996  col_9997  col_9998  col_9999\n",
       "0      0      0      0      0    ...            0         0         0         0\n",
       "1      0      0      0      0    ...            0         0         0         0\n",
       "2      0      0      0      0    ...            0         0         0         0\n",
       "3      0      0      0      0    ...            0         0         0         0\n",
       "4      0      0      0      0    ...            0         0         0         0\n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hashing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score with feature hashing: 0.8526684993463501\n",
      "CPU times: user 8.37 s, sys: 12 ms, total: 8.38 s\n",
      "Wall time: 8.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hashing_logit_score = get_score(logit, X_train_hashing, y_train, X_val_hashing, y_val)\n",
    "print('Logistic Regression score with feature hashing:', hashing_logit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score with feature hashing: 0.82298563375926\n",
      "CPU times: user 27.3 s, sys: 268 ms, total: 27.5 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hashing_rf_score = get_score(rf, X_train_hashing, y_train, X_val_hashing, y_val)\n",
    "print('Random Forest score with feature hashing:', hashing_rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It improves! As we may have guessed, reducing the number of features improves the performance of tree-based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Encoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary encoding involves converting each category into a binary code, for example 2 becomes 11 and 3 becomes 100, and then splitting the resulting binary string into columns. \n",
    "\n",
    "This may be easier to understand with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_0</th>\n",
       "      <th>example_1</th>\n",
       "      <th>example_2</th>\n",
       "      <th>example_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_0  example_1  example_2  example_3\n",
       "0          0          0          0          1\n",
       "1          0          0          1          0\n",
       "2          0          0          1          1\n",
       "3          0          1          0          0\n",
       "4          0          1          0          1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create example dataframe with numbers ranging from 1 to 5:\n",
    "example_df = pd.DataFrame([1,2,3,4,5], columns=['example'])\n",
    "\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "example_binary = BinaryEncoder(cols=['example']).fit_transform(example_df)\n",
    "\n",
    "example_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary encoding is clearly very similar to feature hashing however much more restricted. In practice using feature hashing is often advised over binary encoding due to the control you have over the output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_enc = BinaryEncoder(cols=columns).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary = binary_enc.transform(X_train.reset_index(drop=True))\n",
    "X_val_binary = binary_enc.transform(X_val.reset_index(drop=True))\n",
    "# note: category_encoders implementations can't handle shuffled datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after Binary Encoding: \n",
      " 58\n"
     ]
    }
   ],
   "source": [
    "print('Features after Binary Encoding: \\n', X_train_binary.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score with binary encoding: 0.6410124457048876\n",
      "CPU times: user 308 ms, sys: 12 ms, total: 320 ms\n",
      "Wall time: 302 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "be_logit_score = get_score(logit, X_train_binary, y_train, X_val_binary, y_val)\n",
    "print('Logistic Regression score with binary encoding:', be_logit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score with binary encoding: 0.7455560153361729\n",
      "CPU times: user 432 ms, sys: 248 ms, total: 680 ms\n",
      "Wall time: 390 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "binary_rf_score = get_score(rf, X_train_binary, y_train, X_val_binary, y_val)\n",
    "print('Random Forest score with binary encoding:', binary_rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target encoding is the first of our Bayesian encoders. These are a family of encoders which take information about the target variable into account. Target encoding may refer to an encoder which considers the statistical correlation between the individual categories of a categorical feature. In this tutorial we will only look at target encoders which focus on the relationship between each category and the mean of the target as this is the most commonly used variation of target encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "targ_enc = TargetEncoder(cols=columns, smoothing=8, min_samples_leaf=5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_te = targ_enc.transform(X_train.reset_index(drop=True))\n",
    "X_val_te = targ_enc.transform(X_val.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RESOURCE</th>\n",
       "      <th>MGR_ID</th>\n",
       "      <th>ROLE_FAMILY_DESC</th>\n",
       "      <th>ROLE_FAMILY</th>\n",
       "      <th>ROLE_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.941675</td>\n",
       "      <td>0.972658</td>\n",
       "      <td>0.996885</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.993056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749391</td>\n",
       "      <td>0.828075</td>\n",
       "      <td>0.758443</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.777108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.966019</td>\n",
       "      <td>0.991365</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.910714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.897076</td>\n",
       "      <td>0.977980</td>\n",
       "      <td>0.967211</td>\n",
       "      <td>0.910612</td>\n",
       "      <td>0.931873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.818242</td>\n",
       "      <td>0.570837</td>\n",
       "      <td>0.570837</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.851711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RESOURCE    MGR_ID  ROLE_FAMILY_DESC  ROLE_FAMILY  ROLE_CODE\n",
       "0  0.941675  0.972658          0.996885     0.971549   0.993056\n",
       "1  0.749391  0.828075          0.758443     0.864438   0.777108\n",
       "2  0.966019  0.991365          0.923077     0.946237   0.910714\n",
       "3  0.897076  0.977980          0.967211     0.910612   0.931873\n",
       "4  0.818242  0.570837          0.570837     0.864438   0.851711"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_te.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score with target encoding: 0.8369910052854271\n",
      "CPU times: user 84 ms, sys: 12 ms, total: 96 ms\n",
      "Wall time: 82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "te_logit_score = get_score(logit, X_train_te, y_train, X_val_te, y_val)\n",
    "print('Logistic Regression score with target encoding:', te_logit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score with target encoding: 0.767684128958799\n",
      "CPU times: user 292 ms, sys: 272 ms, total: 564 ms\n",
      "Wall time: 267 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "te_rf_score = get_score(rf, X_train_te, y_train, X_val_te, y_val)\n",
    "print('Random Forest score with target encoding:', te_rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of using the target variable, data-leakage and overfitting is a huge concern. The category_encoders implementation has two out of the box ways of regularizing the encodings, 'smoothing' and 'min_samples_leaf'. These parameters may be treated as hyperparameters.\n",
    "\n",
    "'smoothing' determines the weighting of the individual category's mean with the mean of the entire categorical variable. This is to prevent the influence of unreliable means from categories with low sample sizes.\n",
    "\n",
    "'min_samples_leaf' is the minimum number of samples within a category to take it's mean into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_enc = TargetEncoder(cols=columns, smoothing=8, min_samples_leaf=5).fit(X_train, y_train)\n",
    "\n",
    "X_train_te = targ_enc.transform(X_train.reset_index(drop=True))\n",
    "X_val_te = targ_enc.transform(X_val.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score with target encoding with regularization: 0.8369910052854271\n",
      "CPU times: user 80 ms, sys: 12 ms, total: 92 ms\n",
      "Wall time: 78.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "me_logit_score = get_score(logit, X_train_te, y_train, X_val_te, y_val)\n",
    "print('Logistic Regression score with target encoding with regularization:', me_logit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score with target encoding with regularization: 0.767684128958799\n",
      "CPU times: user 328 ms, sys: 240 ms, total: 568 ms\n",
      "Wall time: 266 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "me_rf_score = get_score(rf, X_train_te, y_train, X_val_te, y_val)\n",
    "print('Random Forest score with target encoding with regularization:', me_rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to regularizing the target encoder is to calculate the statistic relationship between each category and the target variable via a kfold split. This method is currently not available in category_encoders implementation and needs to be written from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create 5 kfold splits:\n",
    "kf = KFold(random_state=17, n_splits=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of data:\n",
    "X_train_te = X_train.copy()\n",
    "X_train_te['target'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_set = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_te):\n",
    "    # Create splits:\n",
    "    train, val = X_train_te.iloc[train_index], X_train_te.iloc[val_index]\n",
    "    val=val.copy()\n",
    "    \n",
    "    # Calculate the mean of each column:\n",
    "    means_list = []\n",
    "    for col in columns:\n",
    "        means_list.append(train.groupby(str(col)).target.mean())\n",
    "    \n",
    "    # Calculate the mean of each category in each column:\n",
    "    col_means = []\n",
    "    for means_series in means_list:\n",
    "        col_means.append(means_series.mean())\n",
    "    \n",
    "    # Encode the data:\n",
    "    for column, means_series, means in zip(columns, means_list, col_means):\n",
    "        val[str(column) + '_target_enc'] = val[str(column)].map(means_series).fillna(means) \n",
    "    \n",
    "    list_of_mean_enc = [str(column) + '_target_enc' for column in columns]\n",
    "    list_of_mean_enc.extend(columns)\n",
    "    \n",
    "    all_set.append(val[list_of_mean_enc].copy())\n",
    "\n",
    "X_train_te=pd.concat(all_set, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encodings to validation set:\n",
    "X_val_te = pd.DataFrame(index=X_val.index)\n",
    "for column, means in zip(columns, col_means):\n",
    "    enc_dict = X_train_te.groupby(column).mean().to_dict()[str(column) + '_target_enc']\n",
    "    X_val_te[column] = X_val[column].map(enc_dict).fillna(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of target encoded columns:\n",
    "list_of_target_enc = [str(column) + '_target_enc' for column in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score with kfold-regularized target encoding: 0.8433289493105047\n",
      "CPU times: user 88 ms, sys: 8 ms, total: 96 ms\n",
      "Wall time: 84.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf_reg_logit_score = get_score(logit, X_train_te[list_of_target_enc], y_train, X_val_te, y_val)\n",
    "print('Logistic Regression score with kfold-regularized target encoding:', kf_reg_logit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score with kfold-regularized target encoding: 0.7747987215170301\n",
      "CPU times: user 412 ms, sys: 240 ms, total: 652 ms\n",
      "Wall time: 356 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf_reg_rf_score = get_score(rf, X_train_te[list_of_target_enc], y_train, X_val_te, y_val)\n",
    "print('Random Forest score with kfold-regularized target encoding:', kf_reg_rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Of Evidence (WOE):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight of evidence (WOE) encoder calculates the natural log of the % of non-events divided by the % of events for each category within a categotical feature. For clarification, the events are referring to the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import WOEEncoder\n",
    "\n",
    "woe_enc = WOEEncoder(cols=columns, random_state=17).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_woe = woe_enc.transform(X_train.reset_index(drop=True))\n",
    "X_val_woe = woe_enc.transform(X_val.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RESOURCE</th>\n",
       "      <th>MGR_ID</th>\n",
       "      <th>ROLE_FAMILY_DESC</th>\n",
       "      <th>ROLE_FAMILY</th>\n",
       "      <th>ROLE_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.834486</td>\n",
       "      <td>0.586900</td>\n",
       "      <td>0.734514</td>\n",
       "      <td>1.496270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.799567</td>\n",
       "      <td>-1.933098</td>\n",
       "      <td>-1.835935</td>\n",
       "      <td>-0.933496</td>\n",
       "      <td>-1.550448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500515</td>\n",
       "      <td>0.215336</td>\n",
       "      <td>-0.317557</td>\n",
       "      <td>0.070517</td>\n",
       "      <td>-0.480253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.732703</td>\n",
       "      <td>-0.477811</td>\n",
       "      <td>-1.394102</td>\n",
       "      <td>-0.464076</td>\n",
       "      <td>-0.180961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.338012</td>\n",
       "      <td>-3.696687</td>\n",
       "      <td>-3.696687</td>\n",
       "      <td>-0.933496</td>\n",
       "      <td>-1.053175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RESOURCE    MGR_ID  ROLE_FAMILY_DESC  ROLE_FAMILY  ROLE_CODE\n",
       "0  0.000000 -0.834486          0.586900     0.734514   1.496270\n",
       "1 -1.799567 -1.933098         -1.835935    -0.933496  -1.550448\n",
       "2  0.500515  0.215336         -0.317557     0.070517  -0.480253\n",
       "3 -0.732703 -0.477811         -1.394102    -0.464076  -0.180961\n",
       "4 -1.338012 -3.696687         -3.696687    -0.933496  -1.053175"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_woe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score with woe encoding: 0.7973584285694204\n",
      "CPU times: user 76 ms, sys: 16 ms, total: 92 ms\n",
      "Wall time: 76 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "woe_logit_score = get_score(logit, X_train_woe, y_train, X_val_woe, y_val)\n",
    "print('Logistic Regression score with woe encoding:', woe_logit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score with woe encoding: 0.7700949022336552\n",
      "CPU times: user 404 ms, sys: 244 ms, total: 648 ms\n",
      "Wall time: 348 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "woe_rf_score = get_score(rf, X_train_woe, y_train, X_val_woe, y_val)\n",
    "print('Random Forest score with woe encoding:', woe_rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, categorical features may be represented in more ways than the traditional one-hot encoding. These representations have different effects on our models and the choice of representation is task specific. Feature hashing and binary encoding offer us ways of encoding the data with lower dimensions which is cheaper computationally as well as being better suited for tree-based models. Target encoding and weight of evidence encoding seem to be much more task specific. \n",
    "\n",
    "Feedback would be appreciated, as well as upvotes! Thank you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading:\n",
    "\n",
    "* [category_encoder documentation](http://contrib.scikit-learn.org/categorical-encoding/)\n",
    "* [weight of evidence](https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html)\n",
    "* [smarter ways of encoding categorical data for machine learning](https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159)\n",
    "* [an exploration of categorical variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
